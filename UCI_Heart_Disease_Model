HealthCare UCI Dataset.


Importing libraries.
import scipy 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder,OneHotEncoder
from sklearn.impute import SimpleImputer,KNNImputer
# Import iterative imputer.
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import warnings
warnings.filterwarnings('ignore')
#models.
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestRegressor
from xgboost import XGBClassifier
#importing machine_learning libraries.
from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score
#Classification Tasks.
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

# Loading the data.
df=pd.read_csv(r"D:\Datasets\heart_disease_uci.csv")

# Printing first five rows of the data.
df.head()

Exploratory Data Analysis.(EDA)
Explore each column.
df.info()

# Checking the data shape.
df.shape

df['id'].min(),df['id'].max()

df['age'].min(),df['age'].max()

# Draw histogram to see the distribution of age column.
sns.histplot(df['age'],kde=True)

# Plot the mean,median,mode of age column using sns.
sns.histplot(df['age'],kde=True)
plt.axvline(df['age'].mean(),color='red')
plt.axvline(df['age'].median(),color='green')
plt.axvline(df['age'].mode()[0],color='blue')

# Print the values of mean,median,mode.
print('Mean',df['age'].mean())
print('Median',df['age'].median())
print('Mode',df['age'].mode()[0])

# Plot the histogram of age column using plotly and coloring by sex.
fig=px.histogram(data_frame=df,x='age',color='sex')
fig.show()

# Find the values of age column grouping by sex column.
df.groupby('sex')['age'].value_counts()

# Find the value of sex column.
df['sex'].value_counts()

# Calculate the percentages of male and female value counts in data.
male_count=726
female_count=194
total_count=male_count+female_count

# Calculate Percentages.
male_percentage=(male_count/total_count)*100
female_percentage=(female_count/total_count)*100

# Displaying results.;
print(f'Male percentage in the data is:{male_percentage:.2f}%')
print(f'Female percentage in the data is:{female_percentage:.2f}%')

# Let's deal with dataset column.
df['dataset'].value_counts()

# Plot the countplot of dataset column.
#sns.countplot(data=df,x='dataset',hue='sex')

# Ploting using plotly.
fig=px.bar(df,x='dataset',color='sex')
fig.show()

<h5>Minimum age to have a heart disease is 28 years.</h5>
<h5>Most of the people get heart disease at the age of 53-54.</h5>
<h5>Most of the male and female get heart disease at the age of 54-55.</h5>
<h5>We have highest number of patients from Cleveland. (304)</h5>
<h5>We have lowest number of people from Switzerland. (123)</h5>
<h5>The highest number of females are from Cleveland. (97)</h5>
<h5>The lowest number of females are from VA long beach. (06)</h5>
<h5>The highest number of females are from Hungary. (212)</h5>
<h5>The lowest number of females are from Switzerland. (113)</h5>
<h5>Restecg is more prone to st-t abnormility in males and in cleveland region.</h5>

# Plot age column using plotly and coloring using dataset.
fig=px.histogram(data_frame=df,x='age',color='dataset')
fig.show()

# Print mean,median,mode of age column groupedby dataset column.
print("Mean of the dataset: ",df.groupby('dataset')['age'].mean)
print("Mean of the dataset: ",df.groupby('dataset')['age'].median)
print("Mean of the dataset: ",df.groupby('dataset')['age'].agg(pd.Series.mode))

df['cp'].value_counts()

<h5>Let's explore cp(Chest Pain) column.</h5>

# Plot age column using plotly and color with cp.
fig=px.histogram(df,x='age',color='cp')
fig.show()

# Countplot on cp grouped by sex.
sns.countplot(df,x='cp',hue='sex')

# Countplot on cp grouped by sex.
sns.countplot(df,x='cp',hue='dataset')

df['trestbps'].describe()

# Draw the histplot of trestbps column.
sns.histplot(df['trestbps'],kde=True)

# Deal with the missing values of trestbps column.
df['trestbps'].isnull().mean()*100

#Impute the missing values of trestbps column with iterative imputer.
imputer=IterativeImputer(max_iter=10,random_state=42)
df['trestbps']=imputer.fit_transform(df[['trestbps']])
print(df['trestbps'].isnull().sum())

# Let's impute other columns with missing values.
df.isnull().sum().sort_values(ascending=False)

# Let's impute other columns with missing values.
df.isnull().sum().sort_values(ascending=False)

df['thal'].value_counts()

print(f'The missing values of thal column are: {df['thal'].isnull().sum()}')

df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)
missing_data_cols = df.isnull().sum()[df.isnull().sum() > 0].index.tolist()
missing_data_cols

categorical_cols = ['thal', 'ca', 'slope', 'exang', 'restecg','fbs', 'cp', 'sex', 'num']
bool_cols = ['fbs', 'exang']
numeric_cols = ['oldpeak', 'thalch', 'chol', 'trestbps', 'age']

# define the function to impute the missing values in thal column
def impute_categorical_missing_data(passed_col):
    
    df_null = df[df[passed_col].isnull()]
    df_not_null = df[df[passed_col].notnull()]

    X = df_not_null.drop(passed_col, axis=1)
    y = df_not_null[passed_col]
    
    other_missing_cols = [col for col in missing_data_cols if col != passed_col]
    
    label_encoder = LabelEncoder()

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    if passed_col in bool_cols:
        y = label_encoder.fit_transform(y)
        
    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_classifier = RandomForestClassifier()

    rf_classifier.fit(X_train, y_train)

    y_pred = rf_classifier.predict(X_test)

    acc_score = accuracy_score(y_test, y_pred)

    print("The feature '"+ passed_col+ "' has been imputed with", round((acc_score * 100), 2), "accuracy\n")

    X = df_null.drop(passed_col, axis=1)

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass
                
    if len(df_null) > 0: 
        df_null[passed_col] = rf_classifier.predict(X)
        if passed_col in bool_cols:
            df_null[passed_col] = df_null[passed_col].map({0: False, 1: True})
        else:
            pass
    else:
        pass

    df_combined = pd.concat([df_not_null, df_null])
    
    return df_combined[passed_col]

def impute_continuous_missing_data(passed_col):
    
    df_null = df[df[passed_col].isnull()]
    df_not_null = df[df[passed_col].notnull()]

    X = df_not_null.drop(passed_col, axis=1)
    y = df_not_null[passed_col]
    
    other_missing_cols = [col for col in missing_data_cols if col != passed_col]
    
    label_encoder = LabelEncoder()

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])
    
    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_regressor = RandomForestRegressor()

    rf_regressor.fit(X_train, y_train)

    y_pred = rf_regressor.predict(X_test)

    print("MAE =", mean_absolute_error(y_test, y_pred), "\n")
    print("RMSE =", mean_squared_error(y_test, y_pred, squared=False), "\n")
    print("R2 =", r2_score(y_test, y_pred), "\n")

    X = df_null.drop(passed_col, axis=1)

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass
                
    if len(df_null) > 0: 
        df_null[passed_col] = rf_regressor.predict(X)
    else:
        pass

    df_combined = pd.concat([df_not_null, df_null])
    
    return df_combined[passed_col]

# impute missing values using our functions
# remove warning
import warnings
warnings.filterwarnings('ignore')

# impute missing values using our functions
for col in missing_data_cols:
    print("Missing Values", col, ":", str(round((df[col].isnull().sum() / len(df)) * 100, 2))+"%")
    if col in categorical_cols:
        df[col] = impute_categorical_missing_data(col)
    elif col in numeric_cols:
        df[col] = impute_continuous_missing_data(col)
    else:
        pass

plt.figure(figsize=(20,20))
colors=['red','green','blue','orange','purple']
for i, col in enumerate(num_cols):
    plt.subplot(3,2,i+1)
    sns.boxplot(x=df[col],color = colors[i % len(colors)])
    plt.title(col)

plt.show()

numerical_cols=['age','trestbps','thalch','oldpeak','ca','chol']

import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Layout: Adjust depending on number of numerical columns
num_cols = len(numerical_cols)
rows = (num_cols + 1) // 2  # 2 plots per row
cols = 2

# Create subplot figure
fig = make_subplots(rows=rows, cols=cols, subplot_titles=numerical_cols)

colors = ['red', 'green', 'blue', 'orange', 'purple']

# Add each boxplot to subplot
for i, col in enumerate(numerical_cols):
    row = (i // 2) + 1
    col_pos = (i % 2) + 1
    fig.add_trace(
        go.Box(y=df[col], name=col, marker_color=colors[i % len(colors)]),
        row=row,
        col=col_pos
    )

# Update layout
fig.update_layout(height=400 * rows, width=1000, title_text="Boxplots of Numerical Columns")

fig.show()

<h2>Splitting data for Machine Learning model.</h2>    

# Assuming your DataFrame is named 'df'
# Identify all categorical columns (including text columns like gender)
categorical_cols = ['sex', 'cp', 'restecg', 'slope', 'thal','dataset']  # Add any others you have
binary_cols = ['fbs', 'exang']  # These should already be 0/1

# Initialize a dictionary to store our encoders
encoders = {}

# Apply LabelEncoder to each categorical column
for col in categorical_cols:
    # Check if the column contains strings
    if df[col].dtype == 'object':
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        encoders[col] = le  # Store the encoder for inverse transform
    else:
        # Column is already numeric, just ensure it's integer
        df[col] = df[col].astype(int)

# Handle binary columns (ensure they're 0/1)
for col in binary_cols:
    df[col] = df[col].astype(int)

# Show the encoded data
print("Encoded DataFrame:")
print(df.head())

# Show the mapping for important columns (like sex)
if 'sex' in encoders:
    print("\nGender encoding:")
    print({i: label for i, label in enumerate(encoders['sex'].classes_)})

# Splitting the data in train test split.
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
# from lightgbm import LGBMClassifier

# impot pipeline
from sklearn.pipeline import Pipeline

# import metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline

# Identify categorical and numerical columns
categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns
numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns

# Preprocessing pipelines
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Full preprocessor
preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_transformer, numerical_cols),
    ('cat', categorical_transformer, categorical_cols)
])

# Create list of models
models = [
    ('Random Forest', RandomForestClassifier(random_state=42)),
    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),
    ('Support Vector Machine', SVC(random_state=42)),
    ('Logistic Regression', LogisticRegression(random_state=42)),
    ('K-Nearest Neighbors', KNeighborsClassifier()),
    ('Decision Tree', DecisionTreeClassifier(random_state=42)),
    ('Ada Boost', AdaBoostClassifier(random_state=42)),
    ('XG Boost', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),
    ('Naive Bayes', GaussianNB())
]

best_model = None
best_accuracy = 0.0

# Iterate over models
for name, model in models:
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    try:
        scores = cross_val_score(pipeline, X_train, y_train, cv=5)
        mean_accuracy = scores.mean()

        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        print("Model:", name)
        print("Cross-validation Accuracy:", mean_accuracy)
        print("Test Accuracy:", accuracy)
        print()

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = pipeline

    except Exception as e:
        print(f"❌ Model {name} failed: {e}")

print("✅ Best Model:", best_model)

<h4>Hyperparameter Tuning</h4>

# Performing Hyperparameter tuning to make the predictions of the model even better.
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
import pandas as pd

# Define preprocessing
numeric_features = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca']
categorical_features = ['sex', 'dataset', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer()),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# Define full pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', GradientBoostingClassifier(random_state=42))
])
# Define hyperparameter grid
param_grid = {
    'model__n_estimators': [50, 100, 150],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 4, 5],
    'model__subsample': [0.8, 1.0]
}

# Grid search with 5-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)

# Fit on training data
grid_search.fit(X_train, y_train)

# Best model and params
print("✅ Best Parameters:\n", grid_search.best_params_)
print("✅ Best CV Accuracy:", grid_search.best_score_)

# Evaluate on test data
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print("✅ Test Accuracy with Best Model:", test_accuracy)

